{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:                  # get to root of project\n",
    "    print(od)\n",
    "except NameError:\n",
    "    od = os.getcwd()\n",
    "    \n",
    "os.chdir(od + '/..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rcParams, plot, scatter\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook, output_file\n",
    "from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label, Ticker, FixedTicker, Arrow, VeeHead\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import bs4 as bs\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pubchempy as pc\n",
    "\n",
    "from collections import Hashable\n",
    "from hashlib import md5\n",
    "\n",
    "import tqdm\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from keras import backend as Kb\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv1D, Conv2D, MaxPool1D, MaxPool2D, UpSampling1D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import regularizers, objectives\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    \"\"\"\n",
    "    Base class for all-purpose autoencoder. VAE, CNN-AE, etc will be built off of this.\n",
    "\n",
    "    Input -> Encoder -> Z Latent Vector -> Decoder -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100, # size of minibatch\n",
    "                 compile_decoder=False # create the decoder. Not necessary for every use case\n",
    "                 ):\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.compile_decoder = compile_decoder\n",
    "        assert Kb.image_dim_ordering() == 'tf', 'Cannot support Theano ordering! Use TF ordering! #tensorflowmasterrace'\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        # self.data_shape = input_shape[1:] # Shape of a single sample\n",
    "        if len(input_shape) == 4:\n",
    "            self.img_rows, self.img_cols, self.img_stacks, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        elif len(input_shape) == 1:\n",
    "            self.img_rows = input_shape[0]  # todo: test this\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "    def rollup_decoder(self, z, z_input, layers_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ae = AE_Dec()\n",
    "        dc = AE_Dec()\n",
    "        last_ae = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            try:\n",
    "                last_ae = layer(last_ae)\n",
    "            except ValueError:\n",
    "                print(layers_list[i])\n",
    "                raise\n",
    "            print(last_ae, last_ae.shape)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        return last_ae, last_dc\n",
    "\n",
    "\n",
    "class AE_Dec(object):\n",
    "    \"\"\"\n",
    "    Dummy object for reasons I can't remember. This may be deprecated.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VAE(Autoencoder):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0, # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=False\n",
    "                 ):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        # Necessary to instantiate this as instance variables such that they can be passed to the loss function (internally), since loss functions are\n",
    "        # all of the form lossfn(y_true, y_pred)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"This is what makes the variational technique happen.\n",
    "        \"\"\"\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        batch = Kb.shape(z_mean)[0]\n",
    "        dim = Kb.int_shape(z_mean)[1]\n",
    "        epsilon = Kb.random_normal(shape=(batch, dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        # We return z_mean + epsilon*sigma^2. Not sure why we use log var\n",
    "        # Basically, create a random variable vector from the distribution\n",
    "        # We are learning a distribution (mu, var) which represents the input\n",
    "        return z_mean + Kb.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        \"\"\"\n",
    "        Custom loss function for VAE. Uses Kullback-Leibler divergence.\n",
    "\n",
    "        Notes from fchollet: binary_crossentropy expects a shape (batch_size, dim) for x and x_decoded_mean,\n",
    "        so we MUST flatten these!\n",
    "        :param x:\n",
    "        :param x_decoded_mean:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = Kb.flatten(x)\n",
    "        x_decoded_mean = Kb.flatten(x_decoded_mean)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * Kb.mean(\n",
    "            1 + self.z_log_var - Kb.square(self.z_mean) - Kb.exp(self.z_log_var), axis=-1)\n",
    "        # Kullbackâ€“Leibler divergence. so many questions about this one single line\n",
    "        return xent_loss + kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_matembed(VAE):\n",
    "    def __init__(self,\n",
    "                 input_shape=(30, 6), latent_dim=2, n_classes=10,  batch_size=100,  \n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=10,  # Size of the dense layer after convs\n",
    "                 n_filters=6,  # Number of filters in the first layer\n",
    "                 px_conv=1,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=True,\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std,\n",
    "                         compile_decoder=compile_decoder)\n",
    "\n",
    "        x_in = Input(batch_shape=(batch_size,) + self.input_shape, name='main_input')\n",
    "        stack = Conv1D(n_filters, px_conv, activation='relu', name='stack1')(x_in)\n",
    "        print(stack.shape)\n",
    "        flat = Flatten()(stack)\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "        \n",
    "        z_mean = Dense(latent_dim)(hidden_1)\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "        \n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        print(input_shape)\n",
    "        output_shape = (batch_size, input_shape[0], input_shape[1], n_filters)\n",
    "        \n",
    "        decoder_restack = Dense(input_shape[0] * input_shape[1], activation='relu')\n",
    "        print(decoder_restack)\n",
    "        decoder_reshape = Reshape(output_shape[1:3])\n",
    "        decoder_mean_squash = Conv1D(n_filters, px_conv, activation='sigmoid')\n",
    "        \n",
    "        \n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        \n",
    "        layers_list = [decoder_hidden, decoder_restack, decoder_reshape, decoder_mean_squash]\n",
    "        \n",
    "        ae, dc = self.rollup_decoder(z, decoder_input, layers_list)\n",
    "        \n",
    "        # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "        # Primary model - VAE\n",
    "        self.model = Model(x_in, ae)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "         # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x_in, self.z_mean)\n",
    "        if self.compile_decoder:\n",
    "            # reconstruct the output from latent space\n",
    "            self.decoder = Model(decoder_input, dc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/home/mike/data/chemistry/compatibility/'\n",
    "files = glob.glob(basedir + 'parsed/' + '*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [pd.read_csv(fn) for fn in files[:2]]\n",
    "for frame in frames:\n",
    "    frame.columns = map(str.lower, frame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "print(frames[n].shape)\n",
    "frames[n].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "print(frames[n].shape)\n",
    "frames[n].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict = {#'fluorocarbon (fkm)': 'fluoroelastomer (fkma)',\n",
    "        '316 stainless steel': 'ss316', 'stainless steel - 316': 'ss316',\n",
    "         '304 stainless steel': 'ss304', 'stainless steel - 304': 'ss304',\n",
    "         'buna': 'buna', 'buna n (nitrile)': 'buna', 'buna-n (nitrile)': 'buna',\n",
    "         'pvdf (kynar)': 'pvdf',\n",
    "         'teflon': 'ptfe',\n",
    "         'acetal (delrin)': 'acetal', 'polyacetal': 'acetal',\n",
    "         'polyurethane': 'urethane',\n",
    "         'cast/ductile iron': 'cast iron',\n",
    "         'polyetherether ketone (peek)': 'peek',\n",
    "         'hastelloy c': 'hastelloy-c',\n",
    "         'epr, epdm': 'epdm',\n",
    "         'santoprene (epdm & polypropylene)': 'santoprene',\n",
    "         'polypropylene': 'pp'\n",
    "        }\n",
    "\n",
    "for frame in frames:\n",
    "    frame.rename(columns=ddict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    frame.set_index('chemical', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    frame.replace('-', np.nan, inplace=True)\n",
    "    print(frame[frame.columns[0]].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_columns(mydf):\n",
    "    labs = {'A': 4, 'B': 3, 'C': 2, 'D': 1, np.NaN: 0}\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(mydf[mydf.columns[0]].replace(labs))\n",
    "    cols = [col.replace(labs) for name, col in mydf.iteritems()]\n",
    "    try:\n",
    "        cols = [lb.transform(col) for col in cols]\n",
    "    except ValueError:\n",
    "        s = set()\n",
    "        for name, col in mydf.iteritems():\n",
    "            s.update(set(col.unique()))\n",
    "        raise ValueError('Could not map labels. Found values: {}'.format(s))\n",
    "    return np.stack(cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = frames[0]\n",
    "ary = get_label_columns(df)\n",
    "ary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ary[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now to create the data set\n",
    "for every chemical C and material M, create a data pair such that the input tensor is nM x 6, the label data for C is filled with 0 for the input, 6th column is hot, and left for the output. Output should be of shape (nC * nM, nM, 6)\n",
    "Also make a NaN mask for when the label to be predicted would be NaN (zeroth column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchem, nmat, ncat = ary.shape # num_chemicals, num_materials, num_categories (incl NaN)\n",
    "x_set = np.zeros((nchem*nmat, nmat, ncat+1))\n",
    "y_set = np.copy(x_set)\n",
    "print(x_set.shape)\n",
    "blank_layer = np.zeros((nmat, 1)) \n",
    "for row in range(nchem):\n",
    "    for col in range(nmat):\n",
    "        idx = row*nmat + col\n",
    "        insert = np.append(ary[row], np.copy(blank_layer), axis=1)\n",
    "        y_set[idx] = np.copy(insert) # not sure if necessary but better safe than sorry\n",
    "        insert[col] = 0\n",
    "        insert[col, ncat] = 1\n",
    "        x_set[idx] = insert\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "# x_set[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_set[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 5\n",
    "input_shape = x_set.shape[1:]\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = VAE_matembed(input_shape=input_shape, latent_dim=latent_dim, batch_size=None,\n",
    "                  compile_decoder=False)\n",
    "vae = ae.model\n",
    "print(vae.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch_size(ary, batch_size=100):\n",
    "    xs = batch_size - (len(ary) % batch_size)\n",
    "    return np.append(ary, ary[:xs], axis=0)\n",
    "\n",
    "def train_test_split_pad(*arrays, batch_size=100, test_size=None, random_state=None):\n",
    "    datasets = model_selection.train_test_split(*arrays, test_size=test_size, random_state=random_state)\n",
    "    return [pad_batch_size(ary, batch_size=batch_size) for ary in datasets]\n",
    "        \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split_pad(x_set, y_set, batch_size=batch_size, random_state=SEED)\n",
    "print( x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0, 'halt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test),\n",
    "        verbose=False,\n",
    "        callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "bs = 10\n",
    "x_in = x_train[n: n+bs]\n",
    "print(x_in.shape)\n",
    "yp = vae.predict(x_in, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
